

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon01.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Measureds">
  <meta name="keywords" content="">
  <title>Salient Object Detection ： A Survey - 云深不知处</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.6","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"copy_btn":true,"image_zoom":{"enable":true},"lazyload":{"enable":true,"onlypost":false},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>云深不知处</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Salient Object Detection ： A Survey">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-01-06 20:15" pubdate>
        2021年1月6日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      21k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      219
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Salient Object Detection ： A Survey</h1>
            
            <div class="markdown-body">
              <p>显著性目标检测</p>
<a id="more"></a>
<p>今天和导师谈了一下毕业设计的方向，导师给我谈到最近在和腾讯做的一个课题合作，关于广告的一个分类，内容主要是：有一个可能是关于包的一个广告，但是在广告中占的篇幅比较小，需要你通过某些算法，去将视频给推荐给有需要的人，或者是以图搜图，来将图片的要点提取出来。要点就是，在没有先验的情况下去提取要点。一般情况下，需要的东西可能不是主体目标</p>
<p>接下来做个综述的翻译吧，听说这个综述的风评不错。</p>
<p>摘要：<br>自然场景中显著物体的检测和分割，通常被称为显著物体检测，已经引起了计算机视觉领域的广泛关注。虽然已经提出了许多模型，并出现了一些应用，但对所取得的成就和存在的问题还缺乏深入的了解。我们的目标是全面回顾显著目标检测的最新进展，并将该领域与其他密切相关的领域，如通用场景分割、目标建议生成和注视预测的显著性进行比较。包括228种出版物，我们调查了i)根源、关键概念和任务，ii)核心技术和主要建模趋势，以及iii)显著对象检测中的数据集和评估度量。我们还讨论了模型性能中的评价指标和数据集偏差等问题，并提出了未来的研究方向。<br>显著目标检测：自下而上显著性；显性显著性；视觉注意；感兴趣区域。</p>
<h3 id="1引言"><a href="#1引言" class="headerlink" title="1引言"></a>1引言</h3><p>人类能够毫不费力且快速地检测到视觉上独特的所谓显著的场景区域(即，前注意阶段)。然后，这些过滤区域被感知并以更精细的细节进行处理，以提取更丰富的高级信息(即，注意力阶段)。认知科学家对这种能力的研究由来已久，最近在计算机视觉领域引起了人们的极大兴趣，主要是因为它有助于找到物体有效表示场景的区域，从而解决复杂的视觉问题，如场景理解。与视觉显著性密切或远程相关的一些主题包括：显著对象检测[41]、注视预测[33、34]、对象重要性[17、157、182]、可记忆性[78]、场景杂乱[169]、视频趣味性[50、69、90、96]、惊奇[80]、图像质量评估[208、209、228]、场景典型性[52、198]、美学[50]和属性[54]。由于篇幅所限，本文不能完全探讨上述所有研究方向。相反，我们只关注显著目标检测，这是一个在过去20年特别是自2007年以来得到很大发展的研究领域[135]。</p>
<h4 id="1-1什么是显著物体检测"><a href="#1-1什么是显著物体检测" class="headerlink" title="1.1什么是显著物体检测"></a>1.1什么是显著物体检测</h4><p>“显著对象检测”或“显著对象分割”在计算机视觉中通常被解释为包括两个阶段的过程：1)检测最显著的对象和2)分割该对象的准确区域。然而，很少有模型明确区分这两个阶段(几乎没有例外，如[20,131,154])。遵循Itti等人的开创性工作[81]和刘等人[139]模型采用显著性概念，将两个阶段同时进行。这些阶段没有单独评估的事实证明了这一点。此外，大多数基于区域的分数已被用于模型评估(例如，精确召回)。第一阶段不必仅限于一个对象。然而，现有的大多数模型都试图分割最显著的对象，尽管它们的预测图可以用来查找场景中的几个对象。第二个阶段属于计算机视觉中的经典分割问题，但不同的是，这里的精度只由最突出的对象决定。<br>一般而言，一致认为，对于良好的显著性检测，模型应至少满足以下三个标准：1)良好的检测：遗漏真实显著区域并将背景错误地标记为显著区域的概率应该较低，2)高分辨率：显著图应具有高或全分辨率，以准确定位显著对象并保留原始图像信息；3)计算效率：作为其他复杂过程的前端，这些模型应快速检测显著区域。</p>
<h4 id="1-2定位显著目标检测"><a href="#1-2定位显著目标检测" class="headerlink" title="1.2定位显著目标检测"></a>1.2定位显著目标检测</h4><p>显著对象检测模型通常旨在仅检测场景中最显著的对象，并分割这些对象的整个范围。另一方面，注视预测模型通常试图预测人类注视的位置，即一小组注视点[27，30]。由于这两种类型的方法输出单个连续值显著图，其中该图中的较高值指示更有可能关注相应的图像像素，因此它们可以互换使用。<br>在注视位置和显著对象之间存在很强的相关性。此外，当被要求选择场景中最显著的对象[20，29，131]时，人类通常会同意彼此中的哪一个。这些如图1所示。<br>与显著对象检测和固定预测模型不同，对象建议模型旨在产生一小组(通常为几百或数千个)重叠的候选对象边界框或区域建议[72]。对象建议生成和显著对象检测是高度相关的。显著性估计在客观性方法中被明确用作线索[10,181]。<br>图像分割，也称为语义场景标注或语义分割，是计算机视觉中研究得很好的领域之一(例如[40])。与输出为二进制地图的显著对象检测不同，这些模型旨在为每个图像像素分配一个标签，该标签是几个类别(如天空、道路和建筑)中的一个。<br>图2说明了这些研究主题之间的差异。</p>
<h4 id="1-3显著物体检测历史"><a href="#1-3显著物体检测历史" class="headerlink" title="1.3显著物体检测历史"></a>1.3显著物体检测历史</h4><p>最早的显著性模型之一，由Itti等人提出。[81]，引发了包括认知心理学、神经科学和计算机视觉在内的多个学科的第一波兴趣。该模型是早期基于中心-环绕机制的一般计算框架和自下而上注意心理学理论(如Treisman和Gelade[192]的特征整合理论，Wolfe等人的引导式搜索模型)的实现。[211]以及Koch和Ullman的计算注意体系结构[105])。在[81]中Itti展示一些他们的模型能够检测场景中空间间断的示例。随后的行为调查(例如，[162])和计算调查(例如，[32])使用注视作为验证显著性假设和比较模型的手段。<br>第二波兴趣随着刘等人的作品而激增。[139,140]和Achanta等人。[5]世卫组织将显著性检测定义为二进制分割问题。这些作者的灵感来自于一些早期努力检测显著区域或原型对象的模型(例如，Ma和Zhang[146]，Liu和Gleicher[133]，以及Walther等人[199])。从那时起，出现了过多的显著性模型。然而，该新定义如何与诸如图像分割(例如，[13,151])、与类别无关的对象建议生成(例如，[10，46，53])、注视预测(例如，[19，26，32，74，93])和对象检测(例如，[55,197])等其他已建立的计算机视觉领域相关的还不太清楚。<br>第三波兴趣最近随着卷积神经网络(CNNs)的复兴而出现[112]，特别是随着完全卷积神经网络的引入[142]。与大多数基于对比线索的经典方法不同[41]，基于CNN的方法消除了对手工特征的需要，减轻了对中心偏向知识的依赖，因此被许多研究人员采用。一个基于CNN的模型通常包含数十万个可调参数和具有可变接受野大小的神经元。具有大<code>感受野</code>的神经元提供了全局信息，可以帮助更好地识别图像中最突出的区域。而具有小感受场的神经元提供可被用于完善顶层生成的显著性图的局部信息。这允许突出显示区域并细化其边界。与手工制作的基于特征的模型相比，这些理想的特性使基于CNN的模型能够获得前所未有的性能。CNN模型正逐渐成为显著目标检测的主流方向。</p>
<h3 id="2最新技术状况调查"><a href="#2最新技术状况调查" class="headerlink" title="2最新技术状况调查"></a>2最新技术状况调查</h3><p>在这一部分中，我们回顾了3个类别的相关工作，包括：1)显著目标检测模型，2)应用，3)数据集。由于一些模型之间的相似性，有时很难在它们之间划出清晰的界限，这里我们主要关注那些对编年史上的主要“浪潮”做出贡献的模型如图3所示。</p>
<h4 id="2-1旧约：经典模式"><a href="#2-1旧约：经典模式" class="headerlink" title="2.1旧约：经典模式"></a>2.1旧约：经典模式</h4><p>在过去的二十年里，已经提出了大量的方法来检测图像中的显著目标。除了少数试图分割感兴趣对象的模型(例如，[11，76，104])外，大多数这些方法的目标是首先从图像中识别显著子集1(即，计算显著图)，然后将其集成以分割整个显著对象。<br>通常，根据它们所利用的类型、操作或属性，可以用两种不同的方式对经典方法进行分类。<br>1)基于块的分析与基于区域的分析。已经利用两种类型的可视子集：块和区域2来检测显著对象。块主要被早期的方法采用，而区域随着超像素算法的引入而变得流行起来。<br>2)内在线索VS外在线索。检测显著物体的一个关键步骤是将它们与干扰物区分开来。为此，一些方法提出仅从输入图像本身提取各种线索来突出目标并抑制干扰(即内在线索)。然而，其他方法认为，内在线索通常不足以区分目标和干扰物，特别是当它们具有共同的视觉属性时。为了解决这个问题，它们结合了外部提示，如用户注释、深度地图或相似图像的统计信息，以便于检测图像中的显著对象。<br>因此，从上面的模型分类来看，有四种组合是可能的。为了更好地组织模型，我们将模型分成三大类：1)具有内在线索的基于块的模型，2)具有内在线索的基于区域的模型，以及3)具有外部线索的模型(包括基于块和基于区域)。在“其他经典模型”子组中将讨论一些可能不太适合这些子组的方法。已查看的型号列在选项卡中。表4(固有模型)，选项卡。表5(外部模型)和Tab.6(其他经典型号)。</p>
<h4 id="2-1-1具有内在线索的基于块的模型"><a href="#2-1-1具有内在线索的基于块的模型" class="headerlink" title="2.1.1具有内在线索的基于块的模型"></a>2.1.1具有内在线索的基于块的模型</h4><p>在这一小节中，我们主要回顾利用从块中提取的内在线索的显著目标检测模型。在Itti等人的开创性工作之后。[81]显著目标检测被广泛定义为捕获场景中的唯一性、独特性或稀有性。<br>在早期的工作[5,133,146]中，唯一性通常是以像素为单位的中心环绕对比度来计算的。Hu等人[75]使用其特征的极变换在2D空间中表示输入图像。然后将图像中的每个区域映射到一维线性子空间。然后，在不实际分割图像的情况下，使用广义主成分分析(GPCA)[196]来估计线性子空间。最后，通过测量区域的特征对比度和几何属性来选择显著区域。Rosin[170]提出了一种检测显著对象的有效方法。他的方法是无参数的，只需要非常简单的像素级操作，如边缘检测、阈值分解和矩保持<br>二值化。Valenti等人[195]提出了一种基于等光线的框架，其中通过线性组合根据曲线度、颜色提升和等中心聚类计算的显著图来估计显著图。<br>在一项有影响力的研究中，Achanta等人。[6]采用调频方法计算全分辨率显著图。像素x的显著性计算如下：<br><img src="https://img-blog.csdnimg.cn/20200608185932722.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中Iµ是图像的平均像素值(例如，rgb/Lab特征)，IωHCI是输入图像的高斯模糊版本(例如，使用5×5内核)。<br>在没有任何关于显著对象大小的先验知识的情况下，出于稳健性目的经常采用多尺度对比度[133，139]。首先构造L层高斯金字塔(如[133,139])。在该金字塔的第l级的图像上的像素x的显著分数(表示为i(L))被定义为：<br><img src="https://img-blog.csdnimg.cn/20200608190005968.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中N(X)是以x为中心的相邻窗口(例如，9×9像素)。即使在这种多尺度增强的情况下，在像素级导出的内在线索通常也太差而不能支持对象分割。为了解决这一问题，一些工作(例如，[5，102，128，139])将对比分析扩展到补丁级别(即，将补丁与其邻居进行比较)。<br>后来在[102]中，Klein和Frtrop提出了一种信息论方法，利用强度、颜色和方向等特征分布之间的Kullback-Leibler散度来计算中心环绕对比度。Li等人[128]将中心-周围对比度表示为对成本敏感的最大边距分类问题。中心补片被标记为阳性样本，而周围的补片都被用作阴性样本。然后，基于训练的代价敏感支持向量机(SVM)，根据中心补丁与周围补丁的可分离性来确定中心补丁的显著性。<br>一些工作将补丁的唯一性定义为它与其他补丁的全局对比[66]。直观地说，如果一个补丁与其最相似的补丁明显不同，那么它就被认为是显著的，而它们的空间距离也被考虑在内。同样，Borji和Itti计算了RGB和LAB颜色空间中的局部和全局斑块稀有度，并将其融合以预测固定位置[26]。在最近的工作[149]中，Margolinet al。基于高维空间中不同的斑块比不明显的斑块更分散的观察结果，提出通过测量斑块到平均斑块的距离来定义斑块的唯一性。为了进一步合并补丁分布，通过将其到平均补丁的路径投影到图像的主分量来测量补丁的唯一性。<br>综上所述，2.1.1部分目标是基于像素或斑块检测仅利用固有线索的显著对象。这些方法通常有两个缺点：i)高对比度边缘通常比显著对象突出，ii)显著对象的边界没有得到很好的保护(特别是在使用大块时)。为了克服这些问题，一些方法提出了基于区域的显著性计算方法。这提供了两个主要优势。首先，区域的数量远远少于块的数量，这意味着开发高效和快速算法的潜力。其次，可以从区域中提取更多的信息特征，从而获得更好的性能。这些基于区域的方法将在下一小节中讨论。</p>
<h4 id="2-1-2具有内在线索的基于区域的模型"><a href="#2-1-2具有内在线索的基于区域的模型" class="headerlink" title="2.1.2具有内在线索的基于区域的模型"></a>2.1.2具有内在线索的基于区域的模型</h4><p>第二子组中的显著性模型采用从使用诸如基于图形的分割[56]、均值漂移[48]、SLIC[7]或Turbopixels[115]等方法生成的图像区域中提取的内在线索。与基于块的模型不同，基于区域的模型通常先将输入图像分割成与亮度边缘对齐的区域，然后再计算区域显著图。<br>作为早期的尝试，在[133]中，区域显著性分数被定义为其所包含像素的平均显著性分数，根据多尺度对比度定义。余等人[220]根据背景和显著区域的观察结果，提出一套规则来确定每个区域的背景分数。显着性被定义为全球区域对比的独特性，有许多方法被广泛研究[43，44，91,175,216]。在[43]中，通过测量目标区域相对于所有其他图像区域之间的全局对比度，引入了基于区域的显著性算法。简而言之，首先将图像分割成N个区域{ri}Ni=1。区域Rii的显著性被测量为：<br><img src="https://img-blog.csdnimg.cn/2020060819014182.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中DR(ri，Rj)捕捉两个区域之间的外观对比度。较高的显著性得分被分配给具有较大全局对比度的区域。Wij是区域Ri和Rj之间的权重项，包含空间距离和区域大小。Perazzi等人[164]证明了如果DR(ri，Rj)被定义为Ri和Rj之间的颜色的欧几里得距离，则可以使用基于有效滤波的技术来计算全局对比度[9]。<br>除了颜色唯一性之外，还考虑了诸如纹理[175]和结构[179]之类的互补线索的显著性用于显著目标检测。Margolin等人[149]建议将区域独特性和斑块显著性，形成显著图。文[44]提出了一种软抽象方法，利用直方图量化和高斯混合模型(GMM)生成一组大规模的感知上均匀的区域，而不是维护每个像素的硬区域索引。通过避免超像素的硬判决边界，这种软抽象提供了更大的空间支持，从而导致更均匀的显著区域。<br>在[86]中，JIiang提出了一种基于多尺度局部区域对比度的方法，该方法计算多个分割的显著值以达到鲁棒性目的，并将这些区域显著值组合以获得像素级的显著图。在[129,216]中采用了使用多层次分割来估计区域显著性的类似想法。Li等人[128]通过构建由超像素的非参数多尺度聚类构建的超图来扩展成对局部对比度，以捕捉区域的内部一致性和外部分离。然后，显著对象检测被演绎为在超图中寻找显著顶点和超边。<br>就唯一性而言，显著对象还可以被定义为特定特征空间中的稀疏噪声，其中输入图像被表示为低秩矩阵[163,177,233]。基本假设是非显著区域(即背景)可以用低秩矩阵来解释，而显著区域可以用稀疏噪声来表示基于这样一个通用的低秩矩阵恢复框架，沈和吴[177]提出了一种统一的方法，将传统的低级特征与高级指导相结合，例如中心优先、人脸优先和颜色优先，以基于学习的特征变换来检测显著对象。相反，邹等人[233]提出利用自下而上分段作为低秩矩阵恢复的指导线索，以达到鲁棒性目的。类似于[177]，在[163]中也采用了高级先验，其中引入了树形结构的稀疏诱导范数正则化来分层描述图像结构，以便均匀地突出整个显著对象。<br>在获取唯一性的同时，越来越多的先验被提出用于显著目标检测。在[139]之前的空间分布意味着越宽的颜色分布在图像中，显著对象包含此颜色的可能性越小。使用高斯模糊核也可以在线性时间内有效地评估超像素的空间分布，这与在Eq中计算全局区域对比度的方式类似。(3)在文献[179]中也考虑了这样的空间分布先验，并根据颜色和结构线索进行了评估。<br>中心优先假设更有可能在图像中心附近找到显著对象。换句话说，背景往往远离图像中心。为此，假设图像的窄边界是背景区域，即伪背景，则对显著对象检测[85、129、210、218]采用背景先验。以此伪背景为参考，区域显著性可以计算为区域与“背景”的对比度。[218]在文[218]中，提出了一个基于无向加权图上流形排序的两阶段显著性计算框架。在第一阶段，基于给予伪背景查询的每一侧的相关性来计算区域显著性得分。在第二阶段，基于给予初始前景的相关性来细化显著性分数。在[129]中，显著性计算被表示为密集和稀疏重建误差假背景。基于背景模板的主成分分析(PCA)基来计算每个区域的密集重建误差，而稀疏重建误差定义为基于背景模板的稀疏表示的残差。这两种类型的重建误差被传播到多个分割上的像素，这些像素将被融合以形成最终的显著图。蒋等人[85]提出了基于吸收马尔可夫链的显著性检测方法，其中瞬时节点和吸收节点分别是图像中心和边缘附近的超像素。每个超像素的显著性被计算为瞬时节点到马尔可夫链的吸收节点的吸收时间。<br>除了这些方法之外，通用对象性优先级4还用于通过利用对象建议来促进显著对象检测[10]。Chang等人[35]通过将客观性和区域显著性融合到图形模型中，提出了一个计算框架。通过迭代最小化编码它们相互作用的能量函数来联合估计这两个项。[89]将区域目标度定义为其包含像素的平均目标值，并将其纳入区域显著性计算。Jia和han[84]根据客观性，通过将每个区域与“软”前景和背景进行比较来计算每个区域的优先显著性。<br>依赖于伪背景假设的显著对象检测有时可能失败，特别是当对象触及图像边界时。为此，在[43，231]中利用了边界连通性先验。直观地说，与背景中的对象相比，显著对象与图像边界的连接要少得多。因此，区域的边界连通性分数可以根据其沿图像边界的长度与该区域的跨越面积之间的比率来估计[231]，该比率可以分别基于该区域到伪背景和其他区域的测地线距离来计算。然后，将这样的边界连通性分数集成到二次目标函数中，以获得最终的优化显著图。值得注意的是，类似的边界连通性先验概念也在[233]中作为分割先验和在[226]中作为环境进行了研究。<br>在[89,126]中已经研究了焦点先验，即突出的对象通常被拍摄在焦点中以吸引更多注意力的事实。蒋等人[89]根据焦点模糊程度计算聚焦度。通过将消焦模糊建模为锐化图像与点扩散函数的卷积，并用高斯核进行近似，通过尺度空间分析，将像素级聚焦投射为估计高斯核的标准差。通过在边界和内部边缘像素传播聚焦和/或清晰度来计算区域聚焦分数。突出度分数最终从唯一性(全局对比度)、客观性和聚焦度分数的非线性组合中导出。<br>基于区域的显著目标检测性能可能会受到分割参数的影响。除了基于多尺度区域[86，128，216]的其他方法外，通过解决[91]中的设施选址问题来提取单尺度潜在显著区域。首先将输入图像表示为超像素上的无向图，然后通过凝聚聚类生成较小的候选区域中心集。在这个集合上，构建了一个子模目标函数来最大化相似度。通过应用贪婪算法，可以迭代优化目标函数以将超像素分组为区域，这些区域的显著值通过区域全局对比度和空间分布进一步测量。<br>贝叶斯框架被用于显著性计算[167,214]，其被表示为在给定输入图像I的情况下估计像素x是前景的后验概率。为了估计显著性先验，首先在检测到的兴趣点周围估计凸包H。将图像I分成内部区域RI和外部区域RO的凸壳H提供了粗略的前景和背景的估计，可以用于似然计算。刘等人[138]采用基于优化的框架来检测显著对象。类似于[214]，凸壳被粗略地估计为将图像划分为纯背景和潜在前景。然后，从图像中学习显著种子，同时从背景区域以及人类先验知识中学习引导图。利用这些线索，引入一个具有Dirichlet边界的一般线性椭圆系统来模拟种子向其他区域的扩散，从而生成显著图。<br>在这一小节回顾的所有模型中，主要有三种类型的区域用于显著性计算。可以使用基于图形的分割算法[56]、均值漂移算法[48]或聚类(量化)来生成大小不同的不规则区域。另一方面，随着超像素算法的最新进展，使用SLIC算法[7]、Turbopixel算法[115]等，大小相当的紧凑区域也成为流行的选择。这两类区域之间的主要区别在于是否应该考虑区域大小的影响。此外，还考虑用于显著性分析的软区域，其中每个像素保持属于所有区域(分量)中的每一个的概率，而不仅仅是硬区域标签(例如，由GMM拟合)。为了进一步增强分割的鲁棒性，可以基于多个分割或以分层的方式生成区域。一般来说，单尺度分割速度较快，而多尺度分割可以提高整体性能。<br>为了衡量地区的显著性，唯一性仍然是最常用的特征，通常以全球和局部地区对比的形式出现。此外，为了提高整体性能，如背景、客观性、焦点和边界连通性，对区域显著性的互补先验进行了越来越多的研究。与基于块的显著性模型相比，这些先验的扩展也是基于区域的显著性模型的主要优势。此外，区域提供更复杂的线索(例如，颜色直方图)以更好地捕捉场景的显著对象，而不是像素和补丁。使用区域定义显著性的另一个好处与效率有关。由于图像中的区域数目远远少于像素数，在区域级别计算显著性可以在生成全分辨率显著图的同时显著降低计算成本。<br>请注意，本小节中讨论的方法仅使用内在线索。在下一个小节中，我们将回顾如何结合外部线索来促进显著对象的检测。</p>
<h3 id="2-1-3具有外在提示的模型"><a href="#2-1-3具有外在提示的模型" class="headerlink" title="2.1.3具有外在提示的模型"></a>2.1.3具有外在提示的模型</h3><p>第三个子组中的模型采用外部线索来辅助图像和视频中显著目标的检测。除了从单个输入图像观察到的视觉提示之外，外部提示还可以从训练图像、相似图像、视频序列、包含共同显著对象的一组输入图像、深度图或光场图像的基本事实注释中导出。在本节中，我们将根据使用的外部线索的类型来回顾这些模型。制表符。5列出具有外部提示的所有模型，其中每个方法都用几个预定义的属性突出显示。<br>具有相似图像的显著目标检测。近年来，随着网络上越来越多的视觉内容的可获得性，通过利用与输入图像视觉上相似的图像来检测显著目标已经被研究。通常，给定输入图像I，首先从大量图像C中检索K个相似图像CI={Ik}Kk=1，可以通过检查这些相似图像来辅助对输入I的显著对象检测。<br>在一些研究中，假设C的显著注释是可用的。例如，Marchesotti等人[148]提出通过一对描述符(f+Ik，f−Ik)来描述每个索引图像Ik，其中f+Ik和f−Ik分别表示根据显著性注释的显著区域和非显著区域的特征描述符(费舍尔向量)。为了计算显著图，输入图像的每个面片px由费舍尔矢量fx描述。根据其与前景和背景区域特征的对比度来计算斑块的显著性{(f+Ik，f−Ik)}Kk=1。<br>或者，基于不同特征对每个图像上的显著性分析的贡献不同的观察，Mai等人[147]建议学习图像特定权重而不是通用权重来融合在不同特征通道上计算的显著图。为此，显著性图的CRF聚合模型仅针对检索到的相似图像进行训练，以说明聚合对单个图像的依赖性5。<br>如果有大规模的图像集合可用，则基于相似图像的显著性效果很好。然而，在这样的集合上，显著性注释是耗时、乏味的，甚至是难以处理的。为了缓解这一问题，一些方法利用未注释的相似图像。对于网络规模的图像集C，Wang等人[204]提出了一种简单有效的显著性估计算法。按像素方向的显著性贴图计算如下：<br><img src="https://img-blog.csdnimg.cn/20200608190651717.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中˜Ik是具有参考I的Ik的几何扭曲版本。主要的洞察力是相似的图像提供对背景区域的良好近似，而显著区域可能不能很好地近似。<br>Siva等人[181]作为抽样问题，提出显著性计算的概率公式。如果从图像CI∪I采样的概率较低，则认为补丁Px是显著的。换言之，如果Px在从相似图像提取的一包补丁中是唯一的，则给予Px较高的显著性分数。<br>共显著目标检测。共显著对象检测算法不是集中在单个图像上计算显著性，而是集中于发现由多个输入图像{ii}Mi=1共享的共同显著对象，即这样的对象可以是具有不同视点的相同对象，也可以是具有相似视觉外观的相同类别的对象。值得注意的是，共显著目标检测算法的关键特征是它们的输入是一组图像，而经典的显著目标检测模型只需要一幅输入图像。<br>共同显著性检测与图像共同分割的概念密切相关，图像共同分割的目的是从多个图像中分割相似对象[16，172]。如[61]所述，共显著和共分段之间存在三个主要区别。首先，共显著检测算法仅关注于检测共同的显著对象，而在共分割方法中也可以分割出相似但不显著的背景[98,158]。其次，一些共分割方法，例如[16]，需要用户输入来指导歧义情况下的分割过程。第三，显著对象检测通常用作预处理步骤，因此比共同分割算法更有效的算法是优选的，尤其是在大量图像上。<br>Li和Ngan[121]提出了一种计算具有某些公共对象的图像对的共显着性的方法。共显着性定义为图像间的对应关系，即对不同区域赋予低显着性值。与文献[36]类似，Chang et al.。建议通过利用跨多个图像的附加重复性属性来计算共显着性。具体地说，像素的共显性分数被定义为传统显着性分数的乘积[66]及其在输入图像上的重复性似然。Fu等人[61]利用单一图像上成熟的全局对比度和空间分布概念，提出了一种基于聚类的共显性检测算法。此外，引入了多个图像上的对应线索来解释显著共现。</p>
<h4 id="2-1-4其他经典模型"><a href="#2-1-4其他经典模型" class="headerlink" title="2.1.4其他经典模型"></a>2.1.4其他经典模型</h4><p>在本节中，我们回顾了旨在通过边界框直接分割或定位显著对象的算法，以及与显著性检测密切相关的算法。一些小节提供了一些不同的分类前面部分涵盖的模型(例如，有监督与无监督)。参见图6。<br>本地化模式。刘等人[139]将二值分割图转换为边界框。最终输出是围绕显著对象的一组矩形。冯等人[57]使用剩余的图像部分将滑动窗口的显著性定义为其合成成本。基于图像的过分割，假设可以在所有滑动窗口中以暴力方式有效地找到的局部最大值对应于显著对象。<br>在许多以前的方法中，基本假设是在输入图像中存在至少一个显著对象。这可能不总是成立的，因为一些背景图像根本不包含显著对象。[205]在[205]中，Wang研究缩略图上显著对象的定位和预测问题。具体地说，通过在多个通道中提取的一组特征来描述每个图像。显著对象的存在被表示为一个二进制分类问题。对于定位，使用训练样本上的随机森林回归器来学习回归函数，以直接输出显著对象的位置。<br>细分模型。显著物体的分割与图形-背景问题密切相关，图形-背景问题本质上是一个试图将显著物体从背景中分离出来的二进制分类问题。余等人。[219]利用由不同的基于对比度的显著性模型生成的不完美显著图的互补特性。具体地说，首先为每个图像生成两个互补的显著图，包括草图状地图和包络状地图。草图状地图可以精确定位最显著对象的部分(即高精度的骨架)，而信封状地图可以大致覆盖整个显著对象(即召回率高的信封)。利用这两个映射图，可以首先从每幅图像中检测出可靠的前景和背景区域，以训练像素分类器。通过使用该分类器对所有其他像素进行标记，可以将显著对象作为一个整体进行检测。该方法在[190]中通过学习互补显著图来进行扩展，以达到显著对象分割的目的。<br>卢等人[144]利用先验的凸性(凹性)进行显著对象分割。该先验假设曲线边界的凸侧上的区域倾向于属于前景。基于这一假设，凹弧首先出现在超像素的轮廓上。对于凹圆弧，其凸性上下文被定义为紧靠圆弧的窗口。然后在具有凹弧的超像素上构建无向权重图，其中顶点之间的权重由图像分层分割中不同尺度上的凹度上下文的总和来确定。最后，执行归一化剪切算法[178]以将显著对象从背景中分离出来。<br>为了更有效地利用上下文线索，王等人。[203]提出将自动上下文分类器[193]集成到迭代能量最小化框架中以自动分割显著对象。自动上下文模型是针对每个像素及其周围环境的多层增强分类器，用于预测其是否与目标概念相关联。后续层构建在前一层的分类基础上。因此，通过分层学习过程，自动利用空间上下文来更准确地分割显著对象。<br>监督模型与非监督模型的对比。现有的基于学习的显著性检测工作大多集中在有监督的场景下，即在给定一组带有基本事实注释的训练样本的情况下学习显著对象检测器。这里的目的是将显著元素与背景元素分开。<br>输入图像中的每个元素(例如，像素或区域)由特征向量f∈rd表示，其中D是特征维度。然后，基于所学习的线性或非线性映射函数f：rd∈R+，将这样的特征向量映射到显著性分数的→R+。<br>可以假设映射函数f是线性的，即，s=wtf，其中w表示特征向量中所有分量的组合权重。刘等人。[139]提出用条件随机场(CRF)模型对显著对象的矩形标注进行训练来学习权重。在最近的工作[143]中，采用大边际框架来学习权重w。<br>然而，由于显著性机制的高度非线性本质，线性映射可能不能很好地捕捉显著性特征。为此，在[97]中扩展了这样的线性积分，其中采用线性支持向量机(SVM)的混合来将特征空间划分成使用分而治之策略线性可分的一组子区域。在每个区域中，学习线性支持向量机、其混合权重和显著性特征的组合参数，以便更好地估计显著性。可替换地，也利用其他非线性分类器，诸如增强型决策树(BDT)[99,153]和随机森林(RF)[87]。<br>一般来说，与启发式方法相比，有监督的方法允许对元素进行更丰富的表示。在有监督的显著目标检测的开创性工作中，刘等人[139]提出了一套包括局部多尺度对比度、区域中心-周围直方图距离和全局颜色空间分布的特征。与仅具有内在线索的模型类似，由于可以在区域级别提取更复杂的描述符，因此用于显著目标检测的基于区域的表示已变得越来越流行。Mehrani和V Eksler[153]通过考虑在图像分类等其他应用中广泛使用的通用区域属性，例如颜色和形状，展示了令人振奋的结果。蒋等人[87]提出了包含区域局部对比度、区域背景和区域类属属性的区域显著性描述符。在[99,143]中，每个区域由一组特征描述，例如局部和全局对比度、背景、空间分布和中心优先。在[143]中也考虑了预先注意的特征。<br>通常，更丰富的表示导致具有更高维度的特征向量，例如，在[87]中D=93，在[99]中D=75。在有大量训练样本的情况下，学习的分类器能够对最具区分性的样本进行分类(自动集成如此丰富的功能并进行提取)。因此，与启发式方法相比，可以期望更好的性能。<br>一些模型使用了非监督技术。[181]在概率框架中，显著性计算被表示为抽样问题。每个图像块的显著程度与其来自所有块的采样概率成正比，所有块都是从输入图像和从未标记图像语料库检索到的相似图像中提取的。[166]利用元胞自动机进行无监督显著目标检测。<br>聚合和优化模型。给定来自不同显著对象检测模型或输入图像的分层分割的M个显著图{Si}Mi=1，聚集模型试图形成更准确的显著图。设Si(X)表示第i个显著图的像素x的显著值。在[22]中，Borji et al.。提出一种标准的显著性聚合方法，具体如下：<br><img src="https://img-blog.csdnimg.cn/2020060819262669.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中Fx=(S1(X)，S2(X)，SM(X))是像素x的显著性得分，并且Sx=1指示x被标记为显著。ζ(·)是实值函数，可以采用以下形式：<br><img src="https://img-blog.csdnimg.cn/20200608192655171.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>受到[22]中聚集模型的启发，Mai等人[147]提出两种聚合解决方案。第一种方案采用像素聚合：<br><img src="https://img-blog.csdnimg.cn/202006081927276.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中λ={λi|i=1，。。。，M+1}是模型参数集，σ(Z)=1/(1+EXP(−z))。然而，要注意的是，这种直接聚集的一个潜在问题是它忽略了相邻像素之间的交互。受[140]的启发，他们提出了第二种解决方案，通过使用CRF聚合多种方法的显著图来捕捉相邻像素之间的关系。在训练数据的基础上，对CRF聚合模型的参数进行了优化。每个像素的显著性是被训练的CRF标记为显著的后验概率。<br>或者，严某等人提出了另一种观点。[216]将在图像的分层分割上计算的显著性图集成到树结构图形模型中，其中每个节点对应于每个分层中的一个区域。由于该树结构，可以使用信任传播来有效地进行显著性推理。实际上，求解三层分层模型相当于对所有单层地图应用加权平均。与朴素的多层融合不同，该分层推理算法可以为每个区域选择最优权重，而不是全局加权。<br>Li等人[123]提出对图像中所有超像素的显著度值进行优化，以同时满足视觉稀有性、中心偏倚和相互关联等几个显著标准。基于区域对之间的相关性(相似度得分)，当考虑所有其他超像素的影响时，用二次规划来优化每个超像素的显著度值。假设表示两个区域rii和rj之间的相关性，则显著值{si}ni=1(表示为s(Ri)，简称Sii)可以通过求解来优化：<br><img src="https://img-blog.csdnimg.cn/20200608192815760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTg3NTE5OQ==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中DDIS是图像对角线长度的一半。DIJAND分别表示到RITO RJ和图像中心的空间距离。在优化过程中，考虑所有其他超像素的影响，采用二次规划的方法对每个超像素的显著度值进行优化。朱等人[231]还采用了类似的基于优化的框架以集成多个前景/背景线索以及平滑项，以自动推断最佳显著性值。<br>采用贝叶斯框架来更有效地整合互补的密集重建误差和稀疏重建误差[129]。在每对区域之间构造完全连接的高斯马尔可夫随机场以加强显著区域之间的一致性[84]，这导致了最终区域显著性得分的有效计算。<br>活动模型。受交互式分割模型(例如，[132,171])的启发，最近通过将SEC中提到的显著性检测的两个阶段显式解耦而出现了一种新的趋势。1.1：a)检测最显著的对象，b)分割它。一些研究提出利用固定预测和分割模型的优点进行主动分割。例如，Mishra等人。[154]组合多个提示(例如，颜色、强度、纹理、立体声和/或运动)以预测注视。然后，在极空间中分割固定点周围的显著对象的“最佳”闭合轮廓。Li等人[131]提出一个由两个部分组成的模型：提出候选区域的分割器和给每个区域一个显著分数的选择器(使用注视预测模型)。类似地，Borji[20]建议首先粗略地定位注视图(或其使用注视预测模型的估计)的峰值处的显著对象，然后使用超像素分割对象。后两种算法采用注释法确定分割性能的上界，提出场景中包含多个对象的数据集，并对注视预测和显著对象分割的内在联系提供了新的见解。<br>视频上的显著对象检测。除了空间信息之外，视频序列还提供时间提示，例如便于显著对象检测的运动。翟和沙[222]首先估计两个连续帧之间的关键点对应关系。运动对比度是基于图像之间的平面运动(单应)计算的，这是通过对点对应应用RANSAC来估计的。刘等人[141]将它们的空间显著特征[139]扩展到由光流算法产生的运动场。以彩色运动场为输入图像，计算局部多尺度对比度、区域中心-周围距离和全局空间分布，并进行线性积分。Rahtu等人[167]通过考虑时间相干性约束，将空间显著性整合到能量最小化框架中。Li等人[18]将基于区域对比度的显著性扩展到时空域。在给定视频序列的帧的过分割的情况下，基于每两个连续帧之间的颜色、纹理和运动特征在无向未加权匹配图上以交互方式估计它们之间的空间和时间区域匹配。不仅在当前帧中，而且在时域中，通过计算区域与周围区域的局部对比度来确定区域的显着性。<br>具有深度的显著目标检测。我们生活在真实的3D环境中，立体内容为引导视觉注意力和理解周围环境提供了额外的深度线索。这一点得到了Lang等人的进一步验证。[111]通过实验分析深度线索对眼睛注视预测的重要性。最近，研究人员已经开始研究如何利用深度线索来检测显著对象[49,160]，其可以间接地从立体图像中捕获或者直接使用深度照相机(例如，Kinect)来捕获。<br>最直接的扩展是采用SEC中引入的广泛使用的假设。2.1.1和2.1.2到深度通道，例如深度图[49,160]上的全局对比度。此外，Niu[160]演示如何利用立体摄影的领域知识来计算显著图。首先将输入图像分割成区域{ri}。在实践中，参加会议的地区通常被分配到小的或零的差异，以最大限度地减少收敛-适应冲突。因此，基于差异的第一种区域显著性被定义为：<br><img src="https://img-blog.csdnimg.cn/20200608193001363.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTg3NTE5OQ==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>光场上的显著目标检测。文献[126]提出了利用光场进行显著性检测的思想。使用专门设计的相机(例如Lytro)捕获的光场实质上是由观看场景的相机网格拍摄的图像阵列。光场数据为显著目标检测提供了两个好处：1)它允许合成一堆聚焦在不同深度的图像，2)它提供场景深度和遮挡的近似值。<br>有了这些额外的信息，Li等人[126]首先利用聚焦度和客观性先验信息稳健地选择背景和前景候选。具体地说，使用具有估计的背景似然分数的图层来估计背景区域。从Mean-Shift算法中选取前景似然分数高的区域作为显著目标候选。最后，利用估计的背景和前景在全聚焦图像上计算基于对比度的显著图。<br>最近在[227]中引入了一个新的具有挑战性的用于光场显著性分析的基准数据集，称为HFUT-Lytro。</p>
<h3 id="2-2新约：基于深度学习的模型"><a href="#2-2新约：基于深度学习的模型" class="headerlink" title="2.2新约：基于深度学习的模型"></a>2.2新约：基于深度学习的模型</h3><p>到目前为止，我们已经回顾的所有方法都旨在使用启发式算法来检测显著目标。虽然手工制作的功能允许实时检测性能，但它们存在几个缺点，这些缺点限制了它们在具有挑战性的场景中捕获显著对象的能力。<br>卷积神经网络(CNNs)[112]作为机器学习中最流行的工具之一，已经被应用于许多视觉问题，如目标识别[108]、语义分割[142]和边缘检测[213]。最近，已经表明当将CNNs[71,118]应用于显著目标检测时，CNNs也是非常有效的。由于其多层次和多尺度的特征，CNN能够在不使用任何先验知识(例如，段级信息)的情况下准确地捕获最显著的区域。此外，多层特征允许CNN更好地定位检测到的显著区域的边界，即使在存在阴影或反射的情况下也是如此。利用CNN强大的特征学习能力，提出了一系列从海量数据中学习显著性表示的算法。这些基于CNN的模型不断刷新几乎所有现有数据集的记录，并正在成为主流解决方案。这一小节的其余部分将致力于审查基于CNN的模型。<br>基于深度学习的显著目标检测模型基本上可以分为两大类。第一类包括使用多层感知器(MLP)进行显著性检测的模型。在这些模型中，输入图像通常被过度分割成单尺度或多尺度的小区域。然后，使用CNN来提取高层特征，然后将这些特征馈送到MLP来确定小区域的显著值。虽然高层特征是从CNN中提取的，但与完全卷积网络(FCNs)不同的是，由于使用了MLP，CNN特征中的空间信息不能被保留。为了突出这些方法与基于FCN的方法之间的区别，我们将它们称为“基于卷积网络的经典方法”(CCN-based)。第二类包括基于“完全卷积网络”(基于FCN)的模型。Long等人的开创性工作。[142]属于这一范畴，旨在解决语义切分问题。由于显著目标检测本质上是一项分割任务，许多研究人员采用了基于FCN的体系结构，因为它们具有保留空间信息的能力。<br>图7显示了基于CNN的显著性模型列表。</p>
<h4 id="2-2-1基于CCN的模型"><a href="#2-2-1基于CCN的模型" class="headerlink" title="2.2.1基于CCN的模型"></a>2.2.1基于CCN的模型</h4><p>基于一维(1D)卷积的方法。作为早期的尝试，等人[71]采用基于区域的方法学习超像素的特征表示。与像素级CNN相比，他们的方法大大降低了计算代价，同时考虑了全局上下文。但是，用平均颜色表示超像素并不能提供足够的信息。此外，使用1D卷积和汇集操作很难完全恢复图像的空间结构，导致预测混乱，特别是当输入图像是复杂场景时。<br>利用本地和全球环境。Wang等人考虑局部和全局信息以更好地检测显著区域[202]。为此，设计了两个子网络，分别用于局部估计和全局搜索。首先使用深度神经网络(DNN-L)学习局部面片特征以确定每个像素的显著性值，然后进行细化操作以捕捉高层客观性。对于全局搜索，他们训练另一个深度神经网络(DNN-G)，使用各种全局对比度特征(如几何信息、全局对比度特征等)来预测每个显著区域的显著值。利用前K个候选区域使用加权和来计算最终的显著图。<br>在[229]与大多数经典的显著目标检测方法类似，在构建多上下文深度学习框架时，同时考虑了局部上下文和全局上下文。输入图像首先被馈送到全局上下文分支以提取全局对比度信息。同时，将作为超像素中心窗口的每个图像块馈送到局部上下文分支以获取局部信息。最后使用二进制分类器通过最小化预测值和地面真实标签之间的统一软最大损失来确定显著性值。采用特定于任务的预训练方案对设计的多上下文模型进行联合优化。<br>Lee等人[63]利用两个子网分别对低级和高级特征进行编码。它们首先提取每个超像素的若干特征，并将它们馈送到由1×1核大小的卷积层堆栈组成的子网络中。然后，使用标准的VGGNet[180]来捕获高级特征。低-和高层次特征被扁平化、拼接，最后馈送到一个两层的MLP中，以判断每个查询区域的显著性。<br>基于边界框的方法。[232]在[232]中，邹某等人提出了一种层次关联的丰富特征(HARF)提取器。为此，首先构建二叉分割树，用于提取分层图像区域并分析所有区域对之间的关系。然后，使用两种不同的方法来计算二叉分割树的叶节点处的区域的两种特征(HARF1和HARF2)。它们利用从RCNN[64]中提取的所有中间特征来捕捉每个图像区域的各种特征。利用这些高维基本特征，计算每个基本特征类型的局部区域对比度和边界区域对比度，以构建更紧凑的表示。最后，采用AdaBoost算法逐步组装弱决策树，构造复合强回归器。<br>Kim等人[100]设计了一种两分支CNN结构，分别获得粗级和细级斑块的粗细表示。选择性搜索[194]方法被用来生成多个区域候选，这些区域候选被视为双区域CNN的输入。将两个分支的特征表示的连接馈送到最终的完全连接层允许预测粗略的连续地图。为了进一步细化粗预测图，采用分层分割的方法来锐化边界，提高空间一致性。<br>[207]在[207]中，Wang 采用Fast R-CNN[64]框架解决显著目标检测问题。首先使用过分割和边缘保持方法将输入图像分割成多尺度区域。对于每个区域，使用外部边界框并将封闭区域馈送到Fast R-CNN。将由多个完全连通的层组成的小网络连接到ROI汇聚层，以确定每个区域的显著性值。最后，提出了一种基于边缘的传播方法来抑制背景区域，使得到的显著图更加均匀。<br>Kim等人[101]训练CNN以预测每个图像块的显著形状。选择性搜索法首先用来定位一堆图像块，每个图像块都作为CNN的输入。在预测每个面片的形状之后，通过累加预测形状类别的掩码与相应概率的乘积并平均所有区域建议来计算中间掩码MiI。为了进一步细化粗略预测图，采用了基于形状类的分层分割显著性检测算法(SCSD-HS)来融合更多的全局信息，这是显著性检测经常需要的。<br>Li等人[120]利用CNN中的高级特征和基于手工方法提取的低级特征。为了增强CNN的泛化能力和学习能力，通过在前两层加入局部响应归一化(LRN)对原R-CNN进行了重新设计。利用选择性搜索方法[194]来生成一堆正方形的补丁作为网络的输入。高层特征和低层特征都被馈送到L1铰链损失的支持向量机中，以帮助判断每个平方区域的显著性。<br>具有多比例输入的模型。Li等人[118]利用预先训练好的CNN作为特征提取器。给出一幅输入图像，他们首先将其分解成一系列不重叠的区域，然后将它们输入到具有三个不同比例输入的CNN中。然后使用三个子网来捕获不同规模的高级功能。<br>从三个尺度上的斑块获得的特征被连接起来，然后馈送到只有两个完全连通的层作为回归变量的小MLP中，以输出在二进制显著性标签上的分布。为了解决过分割不完善的问题，采用了一种基于超像素的显著性细化方法。<br>图8展示了许多流行的基于FCN的体系结构。图9列出了这些体系结构利用的不同类型的信息。<br>讨论。可以看出，上述基于MLP的作品主要依赖于段级信息(例如，图像补丁)和分类网络。这些图像块通常被调整到固定大小，然后被馈送到用于确定每个块的显著性的分类网络。某些模型使用多比例输入来提取多个比例的要素。然而，这样的学习框架不能充分利用高层语义信息。此外，空间信息不能传播到最后完全连通的层，从而导致全局信息丢失。</p>
<h4 id="2-2-2基于FCN的模型"><a href="#2-2-2基于FCN的模型" class="headerlink" title="2.2.2基于FCN的模型"></a>2.2.2基于FCN的模型</h4><p>与运行在补丁级别的基于CCN的模型不同，完全卷积网络(FCN)[142]考虑像素级操作来克服完全连通层造成的问题，例如显著对象边界附近的模糊和不准确的预测。由于模糊神经网络具有良好的性能，近年来出现了大量基于模糊神经网络的显著目标检测模型。<br>Li等人[119]设计具有两个互补分支的CNN：像素级全卷积流(FCS)和分段空间池流(SPS)。FCS在每一级的最后一卷积层之后引入一系列跳跃层，然后将这些跳跃层融合在一起作为FCS的输出。请注意，CNN的一个阶段由共享相同分辨率的所有层组成。SPS利用数据段级别信息进行空间池。最后，FCS和SPS的输出被融合在一起，随后是一个平衡的S形交叉熵损失层，如[213]中所做的那样。<br>刘[137]提出了两个子网络，以从粗到精和从全局到局部的方式生成预测地图。第一子网络可以被认为是编码器，其目标是生成粗略的全局预测。然后，使用由一系列递归卷积层组成的精化子网络将粗预测图从粗尺度细化到细尺度。<br>[187]在[187]中，Don同时考虑区域级显著性估计和像素级显著性预测。对于像素级预测，两条边路径连接到VGGNet的最后两个阶段，然后级联以学习多尺度特征。对于区域级别的估计，首先将每个给定图像过分割成多个超像素，然后使用Clarifai模型[221]进行预测每个超像素的显著程度。将原始图像和两个预测图作为输入到一个小的CNN，生成一个更有说服力的显著图作为最终输出。<br>Tang等人[188]以深度监督网络[113]为例，采用与整体嵌套边缘检测器[213]中类似的架构。与HED不同的是，它们将VGGNet中的原始卷积层替换为递归卷积层，以学习局部、全局和上下文信息。<br>在[110]中，Kuen 提出了一种利用空间变换和递归网络单元的两级CNN。首先使用卷积-反卷积网络来产生初始的粗显着图。空间变换网络[82]用于从原始图像中提取多个子区域，随后是一系列递归网络单元以逐步改进这些子区域的预测。<br>Kruthiventi等人[109]在统一的网络中考虑注视预测和显著目标检测两者。为了捕获多尺度语义信息，引入了四个初始模块[186]，它们分别连接到第二、第四、第五和第六阶段的输出。为了减少上采样的混叠效应，将这四个旁路连接在一起，并通过由两个卷积层组成的小网络。最后，利用S型交叉熵损失对模型进行优化。<br>Li等人[130]考虑联合语义分割和显著对象检测。与FCN工作[142]类似，用卷积层代替VGGNet[180]中原有的两个全连通的层，利用SLIC[8]超像素在空间和特征维度上对超像素之间的拓扑关系进行建模，以克服CNN下采样操作造成的目标边界模糊问题。最后，利用图的拉普拉斯正则化非线性回归将CNN和超像素图的预测组合由粗级变为细级。<br>张某等人[225]使用由CNN提取的显著线索和多级融合机制来检测显著对象。Deeplab[37]架构首先用于捕获高级功能。为了解决Deeplab中步长过大的问题，采用了多尺度二值像素标记方法来改善空间一致性，类似于[118]。<br>Li等人的MSRNet[117]。同时考虑显著对象检测和实例级显著对象分割。多尺度CNN用于同时检测显著区域和轮廓。对于每个比例，上层的要素将与下层的要素合并以逐渐优化结果。为了生成等高线图，使用MCG[14]方法提取少量候选边界框和分割良好的区域，以帮助生成显著的对象实例分割。最后，使用完全连接的CRF模型[107]来精炼空间相干性。<br>侯等人[73]设计基于HED架构的自上而下模型[213]。与将独立的侧路连接到每一级的最后卷积层不同，引入了一系列短连接来在每对侧路之间建立牢固的关系。因此，具有强语义信息的上层特征被传播到下层，帮助它们精确定位显著对象的准确位置。同时，来自较低层的丰富的详细信息使得来自较深层的不规则预测图得以细化。利用一种特殊的融合机制来更好地组合由不同旁路预测的显著图。<br>讨论。上述方法都是基于完全卷积网络的，这使得点对点学习和端到端训练策略成为可能。与基于CCN的模型相比，这些方法更好地利用了卷积运算，大大降低了时间开销。更重要的是，最近利用CNN特征的基于FCN的方法[73,117]远远优于那些使用段级信息的方法。<br>综上所述，利用基于FCN的模型进行显著性检测具有以下3个优点。<br>1)本地与全局。正如中2.2.1提到的。较早的基于CNN的模型显式地(嵌入在单独的网络[118,201,229]中)或隐式地(使用端到端框架)合并了本地和全局上下文信息。这确实与前面几节回顾的许多手工制作的线索背后的设计原则是一致的。然而，基于FCN的方法能够在内部学习局部和全局信息。较低的层倾向于编码更详细的信息，如边缘和精细组件，而较深的层倾向于全局的和语义上有意义的信息。这些特性使基于FCN的网络的性能大大优于传统方法。<br>2)预训微调。精调预先训练的网络的有效性已经在许多不同的应用中得到了证明。该网络通常预先训练在ImageNet数据集[173]上用于图像分类。通过简单的微调，可以将学习到的知识应用于几个不同的目标任务(例如，对象检测[64]、对象定位[161])。在显著对象检测[119,229]中已经采用了类似的策略，并且与从头开始的训练相比已经产生了更好的性能。更重要的是，所学习的特征能够捕获关于对象类别的高级语义知识，因为所使用的网络是针对场景进行预训练和目标分类任务。<br>3)多种多样的体系结构。CNN体系结构由一堆不同的层组成，这些层通过可微函数将输入图像转换为输出地图。FCN的多样性使设计者可以设计适合它们的不同结构。<br>尽管取得了巨大的成功，但基于FCN的模型在几种情况下仍然失败。典型的示例包括具有透明对象的场景、前景和背景之间的低对比度以及复杂的背景，如[73]所示。这就要求在未来开发更强大的体系结构。<br>图10提供了经典模型和基于CNN的模型生成的地图的可视比较。</p>
<h3 id="3-突出目标检测的三个应用"><a href="#3-突出目标检测的三个应用" class="headerlink" title="3.突出目标检测的三个应用"></a>3.突出目标检测的三个应用</h3><p>显著目标检测模型的价值在于其在计算机视觉、图形学和机器人学的许多领域中的应用。显著对象检测模型已被用于若干应用，例如对象检测和识别[21，25，94，155，168，174，176]，图像和视频压缩[68，79]，视频摘要[83，114，145]，照片拼贴/媒体重定向/剪切/缩略图[65，77，200]，图像质量评估[116，134，159]，图像分割[51，92，127，165]，基于内容的图像检索和图像集合浏览[38，58，125，185]。60、62、103、122、183、223]、对象发现[59、95]和人-机器人交互[152、184]。图11显示了示例应用程序。</p>
<h3 id="4-数据集和评估度量"><a href="#4-数据集和评估度量" class="headerlink" title="4.数据集和评估度量"></a>4.数据集和评估度量</h3><h4 id="4-1显著对象检测数据集"><a href="#4-1显著对象检测数据集" class="headerlink" title="4.1显著对象检测数据集"></a>4.1显著对象检测数据集</h4><p>随着越来越多的模型在文献中被提出，更多的数据集被引入来进一步挑战显著性检测模型。早期的尝试旨在收集具有用边界框注释的显著对象(例如，MSRA-A和MSRA-B[139])的图像，而稍后的努力用像素方式的二进制掩码(例如，ASD[6]和DUT-OMRON[218])注释这些显著对象。通常，可以使用精确蒙版进行注释的图像只包含有限的对象(通常是一个)和简单的背景区域。相反，最近已经尝试在复杂和杂乱的背景中收集具有多个对象的数据集(例如，[20，29,131])。正如我们在简介部分中提到的，当同一场景中存在多个候选对象时，需要更复杂的机制来确定最突出的对象。例如，Borji[20]和Li等人[131]使用人体注视图的峰值来确定哪个物体是最突出的物体(即，人类看得最多的物体；参见第1.2节)。<br>Tab中示出了22个显著对象数据集的列表，其中包括20个图像数据集和2个视频数据集。12.请注意，这些数据集中的所有图像或视频帧都用二进制掩码或矩形进行注释。经常要求对象用一个对象来标记图像中的显著对象(例如，[139])，或者注释几个候选对象中最显著的一个(例如，[29])。一些图像数据集还提供在自由观看任务期间收集的每个图像的固定数据。</p>
<h4 id="4-2评估办法"><a href="#4-2评估办法" class="headerlink" title="4.2评估办法"></a>4.2评估办法</h4><p>下面描述用于评估显著对象检测模型的五个普遍同意的、标准的和易于计算的度量。为简单起见，我们使用S来表示归一化到[0,255]的预测显著图，G是显著对象的地面真实二值掩码。对于二进制掩码，我们使用|·|来表示掩码中非零条目的数量。<br>精度召回(PR)A显著图S首先被转换成二进制掩码M，然后通过将M与地面真相G进行比较来计算P个累加和召回：<br><img src="https://img-blog.csdnimg.cn/20200608194639937.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>S的二值化是评价的关键步骤。执行二进制化有三种流行的方法。在第一个解决方案中，Achanta等人。[6]提出了图像相关的自适应阈值对S进行二值化，该阈值的计算是S的平均显著性的两倍：</p>
<p><img src="https://img-blog.csdnimg.cn/20200608194659978.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中W和H分别是显著图S的宽度和高度。<br>对S进行二进制化的第二种方法是使用从0到255的阈值。对于每个阈值，对于每一个(Precision, Recall)分数被计算并用于绘制精确度-召回(PR)曲线。<br>执行二值化的第三种方式是使用类似GrabCut的算法(例如，如[43]中所示)。这里，首先计算PR曲线，并选择导致95%召回率的阈值。利用该阈值，生成初始二进制掩码，该初始二进制掩码可用于初始化迭代GrabCut分割[171]。经过几次迭代之后，可以逐渐细化二进制掩码。<br>F度量通常情况下，无论是P累进还是回忆都不能完全评估显著图的质量。为此，建议将F-测度作为具有非负权重β2的P重现和回忆的加权调和平均值：<br><img src="https://img-blog.csdnimg.cn/20200608194829912.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>正如许多显著目标检测工作(例如，[6])中所建议的那样，β2通常被设置为0.3以更多地加权P个累积量。原因是召回率不如精确度重要(另见[140])。例如，通过将整个地图设置为前景，可以很容易地实现100%的召回。<br>接收器操作特征(ROC)曲线如上所述，当用一组固定阈值将显著图二值化时，可以计算出假阳性(F P R)和真阳性率(T P R)：<br><img src="https://img-blog.csdnimg.cn/20200608194858366.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>其中M和G分别表示二进制掩码M和地面真值G的反面。ROC曲线是通过测试所有可能的阈值绘制的T-P-R与F-P-R的曲线图。<br>ROC曲线(AUC)下的区域虽然ROC是模型性能的2D表示，但AUC将这些信息提取到单个标量中。顾名思义，它被计算为ROC曲线下的面积。完美模型的AUC得分为1，而随机猜测的AUC得分约为0.5。<br>平均绝对误差(MAE)上面介绍的基于重叠的评估措施没有考虑真正的负显著分配，即，正确地标记为不显著的像素。他们倾向于成功地将高显着性分配给显着性像素，但未能检测到非显着性区域的方法。此外，对于某些应用[15]，加权连续显著图的质量可能比二进制掩码更令人关注。为了进行更全面的比较，建议评估连续显著图S和二进制地面实况G之间的平均绝对误差(MAE)，两者都在[0，1]范围内归一化。MAE分数定义为：<br><img src="https://img-blog.csdnimg.cn/20200608194948864.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<h3 id="5-讨论"><a href="#5-讨论" class="headerlink" title="5.讨论"></a>5.讨论</h3><h4 id="5-1设计选择"><a href="#5-1设计选择" class="headerlink" title="5.1设计选择"></a>5.1设计选择</h4><p>在过去的二十年里，人们已经提出了数百种基于经典和深度学习的方法来检测和分割场景中的显著目标，并探索了大量的设计选择。虽然最近取得了很大的成绩，但还有很大的改进空间。我们详细的方法总结(见图4和图5)确实发出了一些关于常用设计选择的明确信息，这对未来算法的设计是有价值的。接下来将讨论它们。</p>
<h4 id="5-1-1启发式与从数据中学习"><a href="#5-1-1启发式与从数据中学习" class="headerlink" title="5.1.1启发式与从数据中学习"></a>5.1.1启发式与从数据中学习</h4><p>早期的方法主要基于启发式(局部或全局)线索来检测显著对象[6，43,164,218]。最近，基于学习算法的显著性模型已经被证明是非常有效的(参见表4和Tab5)。在这些模型中，基于深度学习的方法由于能够从大数据集中学习大量的外部线索而大大优于传统的启发式方法。用于显著对象检测的数据驱动方法似乎具有令人惊讶的良好的泛化能力。然而，一个新出现的问题是，用于显著对象检测的数据驱动的想法是否与这些模型的易用性相冲突。大多数基于学习的方法只在MSRA5K数据集的一小部分上进行训练，并且在所有其他具有相当大差异的数据集上的性能仍然一致地优于其他方法。这表明，在不丧失简单性和易用性优势的情况下，尤其是从应用程序的角度来看，值得进一步探索数据驱动的显著对象检测。</p>
<h4 id="5-1-2手工制作的功能与基于CNN的功能"><a href="#5-1-2手工制作的功能与基于CNN的功能" class="headerlink" title="5.1.2手工制作的功能与基于CNN的功能"></a>5.1.2手工制作的功能与基于CNN的功能</h4><p>第一代基于学习的方法基于许多手工制作的功能。这些方法的一个明显缺点是泛化能力，特别是当应用于复杂的杂乱场景时。此外，这些方法主要依赖过度分割算法，如SLIC[8]，导致大多数具有高对比度分量的显著对象的不完备性。基于CNN的模型在一定程度上解决了这些问题，即使在考虑复杂场景的情况下也是如此。由于CNN具有学习多层特征的能力，因此很容易精确定位显著目标的位置。诸如边缘的低级特征使得能够锐化显著对象的边界，而高级特征允许结合语义信息来标识显著对象。</p>
<h4 id="5-1-3基于CNN的显著性检测的最新进展"><a href="#5-1-3基于CNN的显著性检测的最新进展" class="headerlink" title="5.1.3基于CNN的显著性检测的最新进展"></a>5.1.3基于CNN的显著性检测的最新进展</h4><p>最近提出了各种基于CNN的架构。在这些方法中，有几个有希望的选择可以在未来进一步探索。第一个是对模特进行深度监督。如[73]所示，深度监督网络增强了不同层功能的力量。第二种选择是编解码器体系结构，该体系结构已被许多分割相关任务所采用。这些类型的方法逐渐将高层特征反向传播到较低层，从而允许多层特征的有效融合。另一种选择是利用更强大的基线模型，例如使用非常深的ResNet[70]而不是VGGNet[180]。</p>
<h3 id="5-2数据集偏差"><a href="#5-2数据集偏差" class="headerlink" title="5.2数据集偏差"></a>5.2数据集偏差</h3><p>数据集在显著性检测的快速发展中起着重要作用。一方面，它们提供了大规模的训练数据，并能够比较竞争算法的性能。另一方面，每个数据集都是不受限制的应用程序域的唯一采样，并且包含一定程度的偏差。<br>到目前为止，似乎对数据集的底层结构中存在偏差(即偏度)达成了一致的一致意见。因此，一些研究已经解决了图像数据集中偏差的影响。例如，Torralba和Efros在计算机视觉数据集中识别了三种偏差，即：选择偏差、捕获偏差和负集偏差[191]。选择偏差是由于在数据采集过程中对特定类型图像的偏好造成的。它会在数据集中产生定性上相似的图像。最常用的显著对象基准数据集[6]中的强烈颜色对比度(见[43,131])证明了这一点。因此，优选数据集构建中的两种实践：i)具有独立的图像选择和注释过程[131]，以及ii)首先检测最显著的对象，然后对其进行分割。负集偏差是缺乏丰富、公正的负集的结果，也就是说，人们应该避免将注意力集中在感兴趣的特定图像上，而数据集应该代表整个世界。通过将注释者的个人偏好融入到某些对象类型中，负集偏差可能会影响地面实况。因此，在构建良好的数据集时，鼓励包含各种图像。捕捉偏差传达图像合成对数据集的影响。这种偏向最普遍的一种是在图像的中心区域合成对象的倾向，即中心偏向。数据集中存在的偏差使得定量比较非常具有挑战性，有时甚至具有误导性。例如，由高斯斑点组成的平凡显著性模型在图像中心，得分通常高于许多注视预测模型[28，93,189]。</p>
<h3 id="5-3未来方向"><a href="#5-3未来方向" class="headerlink" title="5.3未来方向"></a>5.3未来方向</h3><p>讨论了构建更有效的模型和基准的几个有前途的研究方向。</p>
<h4 id="5-3-1超越处理单个图像"><a href="#5-3-1超越处理单个图像" class="headerlink" title="5.3.1超越处理单个图像"></a>5.3.1超越处理单个图像</h4><p>本研究中讨论的大多数基准和显著性模型都处理单个图像。不幸的是，较少探索多个输入图像上的显著对象检测，例如，视频序列上的显著对象检测、共显著对象检测、以及深度和光场图像上的显著对象检测。这背后的一个原因是关于这些问题的基准数据集的可获得性有限。例如，如SEC中所述。4、只有两个公开可用的视频显著性基准数据集(主要是卡通和新闻)。对于这些视频，仅为关键帧提供边界框以粗略定位显著对象。多模式数据正变得越来越容易获得和负担得起。集成诸如时空一致性和深度等附加线索将有利于有效的显著目标检测。</p>
<h4 id="5-3-2实例级醒目对象检测"><a href="#5-3-2实例级醒目对象检测" class="headerlink" title="5.3.2实例级醒目对象检测"></a>5.3.2实例级醒目对象检测</h4><p>现有的显著模型是对象不可知的(即，它们不将显著区域分割成对象)。然而，人类拥有在实例级别检测显著对象的能力。实例级显著性在多个应用程序中非常有用，例如图像编辑和视频压缩.<br>用于实例级显著性检测的两种可能的方法如下。第一种涉及使用对象检测或对象建议方法(例如，FastRCNN[64])来提取候选对象边界框的堆栈，然后分割其中的显著对象。第二种方法最初在[117]中提出，它利用边缘信息来区分不同的显著对象。</p>
<h4 id="5-3-3-多功能网络架构"><a href="#5-3-3-多功能网络架构" class="headerlink" title="5.3.3 多功能网络架构"></a>5.3.3 多功能网络架构</h4><p>随着人们对CNNs认识的加深，越来越多有趣的网络结构被开发出来。已经表明，使用高级基线模型和网络体系结构[119]可以显著提高性能。一方面，更深层次的网络确实有助于更好地捕获显著对象，因为它们具有提取高级语义信息的能力。另一方面，除了高层信息外，还应考虑低层特征[73,117]来构建高分辨率显著图。</p>
<h4 id="5-3-4未回答的问题"><a href="#5-3-4未回答的问题" class="headerlink" title="5.3.4未回答的问题"></a>5.3.4未回答的问题</h4><p>剩下的一些问题包括：必须有多少(显著的)对象来表示场景？地图平滑是否会影响分数和模型排名？显著目标检测与其他领域有何不同？在模型评估中解决中心偏差的最好方法是什么？模型和人类之间剩下的差距是什么？与其他相关领域(如注视预测的显著性、场景标记和分类、语义分割、对象检测和对象识别)的协作参与可以帮助回答这些问题，更好地定位该领域，并确定未来的方向。</p>
<h3 id="6总结和结论"><a href="#6总结和结论" class="headerlink" title="6总结和结论"></a>6总结和结论</h3><p>在这篇文章中，我们详尽地回顾了与其密切相关的重要目标检测方面的文献。检测和分割显著对象是非常有用的。图像中的对象自动比背景中的东西(如草、树和天空)吸引更多的注意力。因此，如果我们能够先检测到显著或重要的物体，那么我们就可以在下一阶段进行详细的推理和场景理解。与传统的专用目标检测器相比，显著性模型是通用的，通常速度很快，并且不需要大量的注释。这些属性允许以低成本处理大量图像。<br>探索显著目标检测和注视预测模型之间的联系有助于提高这两种模型的性能。在这一点上，同时提供对人类的显著对象判断和眼睛运动的数据集是非常理想的。进行行为研究，以了解人类如何感知场景中的对象并确定其优先顺序，以及这一概念与语言、场景描述和字幕、视觉问题回答、属性等之间的关系，可以提供无价的洞察力。此外，将更多的注意力放在评估和比较重要的对象模型上，以衡量未来的进展，这一点至关重要。解决数据集偏差，如中心偏差和选择偏差，并转向更具挑战性的图像是很重要的。<br>虽然近几年显著目标检测和分割方法取得了长足的进步，但是对于几乎所有图像都能产生高质量结果的非常健壮的显著目标检测算法仍然缺乏。即使对人类来说，图像中最突出的物体是什么，有时也是一个相当模棱两可的问题。为此，我们有一个一般性的建议：<br>不要问细分市场能为你做什么，而要问你能为细分市场做些什么。-吉滕德拉·马利克(Jitendra Malik)<br>对于构建健壮的算法尤为重要。例如，当处理有噪声的互联网图像时，尽管显著对象检测和分割方法不能保证对单个图像的稳健性能，但是它们的效率和简单性使得能够自动地处理大量的图像。这允许出于可靠性和准确性的目的对图像进行过滤，稳健地运行应用程序[38，42，43，47，77,136]，以及无监督学习[230]。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/01/14/%E5%BF%AB%E6%89%8B%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">快手一面面试</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/01/06/%E4%BA%8B%E4%BB%B6%E5%BE%AA%E7%8E%AF%E6%9C%BA%E5%88%B6/">
                        <span class="hidden-mobile">事件循环机制</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
    <span>Theme is designed by<span> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span> Fluid</span></a> <div id="hitokoto"><script defer>hitokoto()</script></div><script src="https://v1.hitokoto.cn/?encode=js&select=%23hitokoto"></script> 
  </div>
  

  

  
</footer>

<!-- SCRIPTS -->

  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":200})
    NProgress.start()
    document.addEventListener('DOMContentLoaded', function() {
      window.NProgress && window.NProgress.inc();
    })
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      var inputArea = document.querySelector("#local-search-input");
      inputArea.onclick = function () {
        searchFunc(path, 'local-search-input', 'local-search-result');
        this.onclick = null
      }
    })()
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>



</body>
</html>
